{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_IQR(data, col):\n",
    "    \n",
    "    lower_quartile = data[col].quantile(0.25)\n",
    "    upper_quartile = data[col].quantile(0.75)\n",
    "    IQR = upper_quartile - lower_quartile\n",
    "    outlier_thresh = 1.5 * IQR\n",
    "\n",
    "    return data[data[col].between((lower_quartile - outlier_thresh), (upper_quartile + outlier_thresh))]\n",
    "\n",
    "def outliers_zsocre(data, col):\n",
    "    import numpy as np\n",
    "\n",
    "    z = np.abs(stats.zscore(data[col]))\n",
    "    data = data[z <= 3]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover constant/duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo constant columns (no train)\n",
    "colRemove = []\n",
    "for col in train.columns:\n",
    "    if train[col].std() == 0:\n",
    "        colRemove.append(col)\n",
    "train.drop(colRemove, axis=1, inplace=True)\n",
    "\n",
    "# removendo duplicate columns\n",
    "colRemove = []\n",
    "columns = train.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = train[columns[i]].values\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v,train[columns[j]].values):\n",
    "            colRemove.append(columns[j])\n",
    "train.drop(colRemove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar novas features usando o featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load data and put into dataframe\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "\n",
    "import featuretools as ft\n",
    "\n",
    "# Make an entityset and add the entity\n",
    "es = ft.EntitySet(id = 'iris')\n",
    "es.entity_from_dataframe(entity_id = 'data', dataframe = df, \n",
    "                         make_index = True, index = 'index')\n",
    "\n",
    "# Run deep feature synthesis with transformation primitives\n",
    "feature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'data',\n",
    "                                      trans_primitives = ['add_numeric'],\n",
    "                                      agg_primitives=[\"mean\", \"max\", \"min\", \"std\", \"skew\"])\n",
    "\n",
    "print(feature_matrix.head())\n",
    "print(feature_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do label encoder in Day_of_week feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "train.Day_of_week = label.fit_transform(train.Day_of_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(df, cols):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    scaler = Nomalizer()\n",
    "    df[[cols]] = scaler.fit_transform(df[[cols]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index = ['col_name1'], columns = ['col_name2'], values = 'col_name3')\n",
    "df_pivot = df_pivot.fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo a correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criacao de dataframe de correlacao entre as features e a variavel objetivo\n",
    "feature_corr = pd.DataFrame(train.corr()['TARGET'])\n",
    "feature_corr['corr_abs'] = feature_corr['TARGET'].apply(lambda x: x if x>0 else abs(x))\n",
    "feature_corr.columns = ['corr','corr_abs']\n",
    "feature_corr.sort_values(by='corr_abs',ascending=False,inplace=True)\n",
    "feature_corr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando features com correlação maior que 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_predictor_correlated(data):\n",
    "    print(f'Initial features: {len(data.columns)}')\n",
    "    \n",
    "    corr = data.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    \n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= 0.9:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    \n",
    "    selected_columns = data.columns[columns]\n",
    "    data = data[selected_columns]\n",
    "    \n",
    "    print(f'Final features: {len(data.columns)}')\n",
    "    \n",
    "    return data\n",
    "\n",
    "X = remove_predictor_correlated(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variáveis importantes\n",
    "numFeaturesInCombination = 5\n",
    "numCombinations = 400\n",
    "numBestSingleFeaturesToSelectFrom = 20\n",
    "\n",
    "# pegar as features e criar o data frame q vai conter as AUC\n",
    "featuresToUse = singleFeatureTable.iloc[0:numBestSingleFeaturesToSelectFrom-1,0]\n",
    "featureColumnNames = ['feature'+str(x+1) for x in range(numFeaturesInCombination)]\n",
    "featureCombinationsTable = pd.DataFrame(index=range(numCombinations), \n",
    "                                        columns=featureColumnNames + ['combinedAUC'])\n",
    "\n",
    "for combination in range(numCombinations):\n",
    "    # gerar combinações de 5 features\n",
    "    randomSelectionOfFeatures = sorted(np.random.choice(len(featuresToUse), numFeaturesInCombination, \n",
    "                                                        replace=False))\n",
    "    print(f'randomSelectionOfFeatures: {randomSelectionOfFeatures}')\n",
    "    \n",
    "    # pegar os nomes das features e adicionar no data frame q conterá as combinações e o AUC\n",
    "    combinationFeatureNames = [featuresToUse[x] for x in randomSelectionOfFeatures]\n",
    "    print(f'combinationFeatureNames: {combinationFeatureNames}')\n",
    "    for i in range(len(randomSelectionOfFeatures)):\n",
    "        featureCombinationsTable[combination,featureColumnNames[i]] = combinationFeatureNames[i]\n",
    "        print(f'giro {i}, featureCombinationsTable: {featureCombinationsTable}')\n",
    "\n",
    "    # pegar os valores da features que useremos\n",
    "    trainInputFeatures = X_train[:,combinationFeatureNames]\n",
    "    validInputFeatures = X_valid[:,combinationFeatureNames]\n",
    "    \n",
    "    # treinar o modelo\n",
    "    modelCombination.fit(trainInputFeatures, y_train)\n",
    "    \n",
    "    # calcula e guarda os resultados da AUC\n",
    "    validAUC = auc(y_valid, modelCombination.predict_proba(validInputFeatures)[:,1])        \n",
    "    featureCombinationsTable[combination,'combinedAUC'] = validAUC\n",
    "\n",
    "validAUC = np.array(featureCombinationsTable.loc[:,'combinedAUC'])\n",
    "print(\"(min,max) AUC = (%.4f,%.4f).\" % (validAUC.min(),validAUC.max()))\n",
    "\n",
    "# mostra as 20 melhores combinações\n",
    "featureCombinationsTable = featureCombinationsTable.sort_values(by='combinedAUC', axis=0, \n",
    "                                                                ascending=False).reset_index(drop=True)\n",
    "featureCombinationsTable.iloc[:20,:]\n",
    "\n",
    "# pegando as TOP 5\n",
    "feature = []\n",
    "for i in range(5):\n",
    "    feature.append(list(featureCombinationsTable.iloc[i,:-1].values))\n",
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inpvar\n",
    "y = outvar\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression, f_classif\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "k_vs_score = []\n",
    "\n",
    "for k in range(2,X.shape[1]+1,2):\n",
    "    selector = SelectKBest(score_func=f_regression, k=k) # f_classif para modelos de classificação\n",
    "\n",
    "    Xtrain2 = selector.fit_transform(X_train, y_train)\n",
    "    Xval2 = selector.transform(X_test)\n",
    "\n",
    "    mdl = Lasso(alpha=0.08885877289587728, normalize=True)\n",
    "  \n",
    "    mdl.fit(Xtrain2, y_train)\n",
    "\n",
    "    y_pred = mdl.predict(Xval2)\n",
    "\n",
    "    errors = metricas(y_test,y_pred)\n",
    "    errors['k'] = k\n",
    "\n",
    "    print(errors)\n",
    "\n",
    "    k_vs_score.append(errors)\n",
    "\n",
    "k_errors = pd.DataFrame(k_vs_score)\n",
    "k_errors.set_index('k', inplace = True)\n",
    "\n",
    "# plotar os gráficos\n",
    "g1 = k_errors.iloc[:,:-1].plot(figsize=(10,7))\n",
    "g2 = k_errors['medae'].plot(figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treina com o melhor k e pega o nome das colunas\n",
    "selector = SelectKBest(score_func=f_regression, k=best_k)\n",
    "selector.fit(X_train, y_train)\n",
    "inpvar_selected_columns = X_train.iloc[:, selector.get_support()].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inpvar\n",
    "y = outvar\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "k_vs_score = []\n",
    "\n",
    "for k in range(2, X_train.shape[1], 2):\n",
    "    \n",
    "    selector_model = Lasso(alpha=0.08885877289587728, normalize=True)\n",
    "    selector = SelectFromModel(selector_model, max_features=k, threshold=-np.inf)\n",
    "\n",
    "    selector.fit(X_train, y_train)\n",
    "\n",
    "    Xtrain2 = selector.transform(X_train)\n",
    "    Xval2 = selector.transform(X_test)\n",
    "\n",
    "    mdl = Lasso(alpha=0.08885877289587728, normalize=True)\n",
    "\n",
    "    mdl.fit(Xtrain2, y_train)\n",
    "\n",
    "    y_pred = mdl.predict(Xval2)\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] < 0:\n",
    "            y_pred[i] = abs(y_pred[i])\n",
    "\n",
    "    errors = metricas(y_test,y_pred)\n",
    "    errors['k'] = k\n",
    "\n",
    "    print(errors)\n",
    "\n",
    "    k_vs_score.append(errors)\n",
    "\n",
    "k_errors = pd.DataFrame(k_vs_score)\n",
    "k_errors.set_index('k', inplace = True)\n",
    "\n",
    "k_errors.iloc[:,:-1].plot(figsize=(10,7))\n",
    "\n",
    "print(f'min medae: {k_errors.medae.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treina com o melhor k e pega o nome das colunas\n",
    "selector = SelectFromModel(selector_model, max_features=k, threshold=-np.inf)\n",
    "selector.fit(X_train, y_train)\n",
    "inpvar_selected_columns = X_train.iloc[:, selector.get_support()].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = train.drop('lights', axis=1)\n",
    "y = train[['lights']]\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# get just importance feature\n",
    "features = X.columns\n",
    "im = importance < 0.03\n",
    "cols = list(features[im].values)\n",
    "cols.append('lights')\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machile Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(y_val, y_pred):\n",
    "    from sklearn.metric import confusion_matrix\n",
    "\n",
    "    conf = confusion_matrix(y_cv, y_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp / tp+fn\n",
    "    acc = (tp+tn) / (tn+fp+fn+tp)\n",
    "    prec = tp / (tp+fp)\n",
    "    \n",
    "    return {'Confusion Matrix': conf, 'Sensibility': sens, 'Accuracy': acc, 'Precision': prec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_methods(method, model, data):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = data.drop('TARGET', axis=1)\n",
    "    y = data[['TARGET']]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "       \n",
    "    p = model.predict(X_val)\n",
    "    param = model.get_params()\n",
    "    results = metricas(y_val, p)\n",
    "    \n",
    "    print(method)\n",
    "    print(results)\n",
    "    \n",
    "    results['Parametros'] = param\n",
    "    results['Modelos'] = model\n",
    "    results['Previsao'] = p\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = 'None' # None, ZScore, IQR, ZScore&IQR\n",
    "obs = ''\n",
    "#overall_results = pd.read_excel('result.xlsx')\n",
    "overall_results = pd.DataFrame()\n",
    "\n",
    "methods = ['logistica_reg', 'naiveBayes', 'randomForest', \n",
    "           'svc', 'lgbm', 'xgb']\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    if method == 'logistica_reg':\n",
    "        model = LogisticRegression()\n",
    "  \n",
    "    elif method =='naiveBayes':\n",
    "        model = GaussianNB()\n",
    "\n",
    "    elif method == 'randomForest':\n",
    "        model = RandomForestClassifier()\n",
    "        \n",
    "    elif method == 'svc':\n",
    "        model = SVC()\n",
    "        \n",
    "    elif method == 'lgbm':\n",
    "        model = LGBMClassifier()\n",
    "            \n",
    "    elif method == 'xgb':        \n",
    "        model = XGBClassifier()\n",
    "            \n",
    "    results = class_methods(method,model,train,params)\n",
    "    \n",
    "    inf = pd.DataFrame({'metodo':method,\n",
    "                        'outliers': outliers,\n",
    "                        'observações': obs,\n",
    "                       })\n",
    "    \n",
    "    result_all = pd.concat([inf,results], axis=1)\n",
    "    overall_results = pd.concat([overall_results,result_all])\n",
    "\n",
    "overall_results.to_excel('result.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(y_test, y_pred):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import median_absolute_error\n",
    "    from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    rmsle = (np.sqrt(mean_squared_log_error(y_test, y_pred)))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return {'rmse': rmse, 'rmsle': rmsle, 'mae': mae, 'medae': medae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_methods(method, model, data):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = data.drop('TARGET', axis=1)\n",
    "    y = data[['TARGET']]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    if norm:\n",
    "        cols = X_train.columns[:]\n",
    "        X_train = normalizer(X_train, cols)\n",
    "        \n",
    "    model.fit(X_train, y_train)    \n",
    "    p = model.predict(X_val)\n",
    "    param = model.get_params()\n",
    "    results = metricas(y_val, p)\n",
    "    results_cross = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "    print(method)\n",
    "    print(results)\n",
    "    print('Cross-Validation', np.mean(results_cross))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = 'None' # None, ZScore, IQR, ZScore&IQR\n",
    "obs = ''\n",
    "NORM = 0\n",
    "#overall_results = pd.read_excel('result.xlsx')\n",
    "overall_results = pd.DataFrame()\n",
    "\n",
    "methods = ['linear_regression','lasso','ridge','random_forest',\n",
    "           'decision_tree','gradient_boosting','xgboost','elastic_net']\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    if method == 'linear_regression':\n",
    "        model = LinearRegression()\n",
    "\n",
    "    elif method == 'lasso':\n",
    "        model = Lasso()\n",
    "\n",
    "    elif method == 'ridge':\n",
    "        model = Ridge()\n",
    "        \n",
    "    elif method == 'random_forest':\n",
    "        reg = RandomForestRegressor()\n",
    "        \n",
    "    elif method == 'decision_tree':\n",
    "        reg = DecisionTreeRegressor()\n",
    "            \n",
    "    elif method == 'gradient_boosting':        \n",
    "        model = GradientBoostingRegressor()\n",
    "            \n",
    "    elif method == 'xgboost':\n",
    "        model = XGBRegressor(objective=\"reg:squarederror\")\n",
    "    \n",
    "    elif method == 'elastic_net':        \n",
    "        reg = ElasticNet()\n",
    "        \n",
    "    results = reg_methods(method,model,train,NORM)\n",
    "    \n",
    "    results['Metodo'] = method\n",
    "    results['Observaoções'] = obs\n",
    "    results['Outliers'] = outliers\n",
    "    \n",
    "    results_df = pd.DataFrame(results, index=[0])\n",
    "    \n",
    "    overall_results = pd.concat([overall_results,results_df])\n",
    "    \n",
    "overall_results.to_excel('result.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Aprendizagem Não Supervisionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
